{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899948d5",
   "metadata": {},
   "source": [
    "\n",
    "# Fader Network - Démonstration\n",
    "\n",
    "Ce notebook permet de reproduire l'intégralité du code et des expériences décrites dans le dépôt **Fader Network**.\n",
    "Vous trouverez :\n",
    "1. La configuration des dépendances\n",
    "2. La préparation des datasets\n",
    "3. Le chargement des modèles (autoencodeur et discriminateur)\n",
    "4. La visualisation des données\n",
    "5. L'évaluation et la prédiction\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dac82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importation des bibliothèques nécessaires\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from model import AutoEncoder, Discriminator, CelebADataset\n",
    "\n",
    "# Configuration du dispositif GPU/CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de : {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0a1ba",
   "metadata": {},
   "source": [
    "\n",
    "## Chargement des datasets\n",
    "Nous utilisons les images CelebA et leurs attributs pour créer trois datasets : entraînement, validation et test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe46d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Répertoires de données\n",
    "img_dir = \"datas/img_align_celeba/img_align_celeba/\"\n",
    "attr_path = \"datas/list_attr_celeba.csv\"\n",
    "partition_path = \"datas/list_eval_partition.csv\"\n",
    "\n",
    "# Chargement des datasets\n",
    "batch_size = 32\n",
    "\n",
    "dataset_train = CelebADataset(img_dir, attr_path, partition_path, transform, split=\"train\")\n",
    "dataset_val = CelebADataset(img_dir, attr_path, partition_path, transform, split=\"valid\")\n",
    "dataset_test = CelebADataset(img_dir, attr_path, partition_path, transform, split=\"test\")\n",
    "\n",
    "print(f\"Nombre d'images dans le dataset d'entraînement : {len(dataset_train)}\")\n",
    "print(f\"Nombre d'images dans le dataset de validation : {len(dataset_val)}\")\n",
    "print(f\"Nombre d'images dans le dataset de test : {len(dataset_test)}\")\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4985b23",
   "metadata": {},
   "source": [
    "\n",
    "## Chargement des modèles\n",
    "Nous chargeons l'autoencodeur et le discriminateur avec leurs poids sauvegardés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chargement des modèles\n",
    "autoencoder = AutoEncoder().to(device)\n",
    "autoencoder.load_state_dict(torch.load(\"sexe/train_sexe70epoch/autoencoder.pth\", map_location=device))\n",
    "autoencoder.eval()\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator.load_state_dict(torch.load(\"sexe/train_sexe70epoch/discriminator.pth\", map_location=device))\n",
    "discriminator.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17eb77",
   "metadata": {},
   "source": [
    "\n",
    "## Visualisation d'une image avec ses attributs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0466630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def affiche_image_et_attributs(dataset, idx):\n",
    "    image, attributs = dataset[idx]\n",
    "    attribute_names = dataset.get_attribute_names()\n",
    "\n",
    "    plt.imshow(image.permute(1, 2, 0).numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Attributs associés à l'image :\")\n",
    "    for i, attr in enumerate(attributs):\n",
    "        print(f\"{attribute_names[i]} : {'Oui' if attr.item() == 1 else 'Non'}\")\n",
    "\n",
    "# Affichage d'une image du dataset de validation\n",
    "affiche_image_et_attributs(dataset_val, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c944d65",
   "metadata": {},
   "source": [
    "\n",
    "## Évaluation du modèle sur le dataset de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7172a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def evaluer_autoencodeur(dataloader):\n",
    "    ae_loss_total = 0\n",
    "    disc_loss_total = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_images, batch_attrs in tqdm(dataloader):\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_attrs = batch_attrs.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = autoencoder.encoder(batch_images)\n",
    "            reconstructed_images = autoencoder.decoder(z, batch_attrs)\n",
    "            ae_loss = criterion(reconstructed_images, batch_images)\n",
    "\n",
    "            disc_pred_real = discriminator(batch_images)\n",
    "            disc_pred_fake = discriminator(reconstructed_images)\n",
    "            disc_loss = 0.5 * (criterion(disc_pred_real, torch.ones_like(disc_pred_real)) +\n",
    "                               criterion(disc_pred_fake, torch.zeros_like(disc_pred_fake)))\n",
    "\n",
    "            ae_loss_total += ae_loss.item()\n",
    "            disc_loss_total += disc_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    ae_loss_avg = ae_loss_total / num_batches\n",
    "    disc_loss_avg = disc_loss_total / num_batches\n",
    "\n",
    "    print(f\"Perte moyenne de l'autoencodeur sur le dataset de test : {ae_loss_avg:.4f}\")\n",
    "    print(f\"Perte moyenne du discriminateur sur le dataset de test : {disc_loss_avg:.4f}\")\n",
    "\n",
    "# Évaluation du modèle sur le dataset de test\n",
    "evaluer_autoencodeur(dataloader_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1779423f",
   "metadata": {},
   "source": [
    "\n",
    "## Prédiction sur une image spécifique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predit_image(image_path):\n",
    "    pil_img = Image.open(image_path).convert(\"RGB\")\n",
    "    plt.imshow(pil_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    input_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        z = autoencoder.encoder(input_tensor)\n",
    "        output_tensor = autoencoder.decoder(z, torch.ones((1, 1), device=device))\n",
    "\n",
    "    output_image = output_tensor.squeeze(0).cpu().permute(1, 2, 0)\n",
    "    plt.imshow(output_image.numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Prédiction sur une image spécifique\n",
    "predit_image(\"datas/img_align_celeba/img_align_celeba/000001.jpg\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
